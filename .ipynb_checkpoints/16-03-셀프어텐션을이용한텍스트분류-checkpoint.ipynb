{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26de5b6e",
   "metadata": {},
   "source": [
    "# 16. 트랜스포머(Transformer)\n",
    "\n",
    "## 3) 셀프 어텐션을 이용한 텍스트 분류(Multi-head Self Attention for Text Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c808f",
   "metadata": {},
   "source": [
    "### 1. 멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdb4ab",
   "metadata": {},
   "source": [
    "우선 트랜스포머의 인코더의 첫번째 서브층인 멀티 헤드 어텐션층을 클래스로 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e331a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5fd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102776b",
   "metadata": {},
   "source": [
    "### 2. 인코더 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c906a",
   "metadata": {},
   "source": [
    "멀티 헤드 어텐션에 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 추가하여 인코더 클래스를 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd8f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embedding_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 헤드 어텐션\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n",
    "        ffn_output = self.ffn(out1) # 두번째 서브층 : 포지션 와이즈 피드 포워드 신경망\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d63c86",
   "metadata": {},
   "source": [
    "### 3. 포지션 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44e7cf",
   "metadata": {},
   "source": [
    "앞서 트랜스포머를 설명할 때는 포지셔널 인코딩을 사용하였지만, 이번에는 위치 정보 자체를 학습을 하도록 하는 포지션 임베딩이라는 방법을 사용합니다.\n",
    "\n",
    "포지션 임베딩은 임베딩 층(Embedding layer)를 사용하되, 위치 벡터를 학습하도록 하므로 임베딩 층의 첫번째 인자로 단어 집합의 크기가 아니라 문장의 최대 길이를 넣어줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6622b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747c9a3",
   "metadata": {},
   "source": [
    "### 4. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33832f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 25000\n",
      "테스트용 리뷰 개수 : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용\n",
    "max_len = 200  # 문장의 최대 길이\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n",
    "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ce6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162993b",
   "metadata": {},
   "source": [
    "### 5. 트랜스포머를 이용한 IMDB 리뷰 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c376d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32  # 각 단어의 임베딩 벡터의 차원\n",
    "num_heads = 2  # 어텐션 헤드의 수\n",
    "dff = 32  # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(max_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n",
    "x = transformer_block(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c890ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 21s 24ms/step - loss: 0.3753 - accuracy: 0.8260 - val_loss: 0.2917 - val_accuracy: 0.8782\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.1978 - accuracy: 0.9250 - val_loss: 0.3256 - val_accuracy: 0.8704\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.3256 - accuracy: 0.8704\n",
      "테스트 정확도: 0.8704\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, batch_size=32, \n",
    "                    epochs=2, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba96e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
