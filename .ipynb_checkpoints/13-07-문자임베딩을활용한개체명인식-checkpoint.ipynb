{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0179402b",
   "metadata": {},
   "source": [
    "# 13. 태깅 작업(Tagging Task)\n",
    "\n",
    "## 7) 문자 임베딩(Character Embedding) 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989defa",
   "metadata": {},
   "source": [
    "개체명 인식기의 성능을 올리기 위한 방법으로 문자 임베딩을 워드 임베딩과 함께 입력으로 사용하는 방법이 있습니다. 워드 임베딩에 문자 임베딩을 연결(concatenate)하여 성능을 높여봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2127d80",
   "metadata": {},
   "source": [
    "### 1. 문자 임베딩(Char Embedding)을 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dfeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f804de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data['Word'] = data['Word'].str.lower()\n",
    "\n",
    "# 하나의 문장에 등장한 단어와 개체명 태깅 정보끼리 쌍(pair)으로 묶는 작업을 수행\n",
    "func = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\n",
    "tagged_sentences=[t for t in data.groupby(\"Sentence #\").apply(func)]\n",
    "\n",
    "# 각 순서에 등장하는 원소들끼리 묶어줍니다.\n",
    "sentences, ner_tags = [], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_info = zip(*tagged_sentence) \n",
    "    sentences.append(list(sentence))\n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d8d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 31819\n",
      "개체명 태깅 정보 집합의 크기 : 18\n"
     ]
    }
   ],
   "source": [
    "# 모든 단어를 사용하며 인덱스 1에는 단어 'OOV'를 할당.\n",
    "src_tokenizer = Tokenizer(oov_token='OOV')\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# 태깅 정보들은 내부적으로 대문자를 유지한 채 저장\n",
    "tar_tokenizer = Tokenizer(lower=False)\n",
    "tar_tokenizer.fit_on_texts(ner_tags)\n",
    "\n",
    "vocab_size = len(src_tokenizer.word_index) + 1\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\n",
    "\n",
    "X_data = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_data = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1650d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = src_tokenizer.word_index\n",
    "index_to_word = src_tokenizer.index_word\n",
    "\n",
    "ner_to_index = tar_tokenizer.word_index\n",
    "index_to_ner = tar_tokenizer.index_word\n",
    "index_to_ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0995fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_data = pad_sequences(X_data, padding='post', maxlen=max_len)\n",
    "y_data = pad_sequences(y_data, padding='post', maxlen=max_len)\n",
    "\n",
    "X_train, X_test, y_train_int, y_test_int = train_test_split(X_data, y_data, \n",
    "                                                            test_size=.2, \n",
    "                                                            random_state=777)\n",
    "\n",
    "y_train = to_categorical(y_train_int, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test_int, num_classes=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3573cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합 : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '\\x85', '\\x91', '\\x92', '\\x93', '\\x94', '\\x96', '\\x97', '\\xa0', '°', 'é', 'ë', 'ö', 'ü']\n"
     ]
    }
   ],
   "source": [
    "# char_vocab 만들기\n",
    "words = list(set(data[\"Word\"].values))\n",
    "chars = set([w_i for w in words for w_i in w])\n",
    "chars = sorted(list(chars))\n",
    "print('문자 집합 :',chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c62f2",
   "metadata": {},
   "source": [
    "이렇게 얻은 문자 집합으로부터 문자를 정수로 변환할 수 있는 딕셔너리인 char_to_index와 반대로 정수로부터 문자를 얻을 수 있는 딕셔너리인 index_to_char를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd08205",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char_to_index[\"OOV\"] = 1\n",
    "char_to_index[\"PAD\"] = 0\n",
    "\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edc4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_char = 15\n",
    "\n",
    "# 문자 시퀀스에 대한 패딩하는 함수\n",
    "def padding_char_indice(char_indice, max_len_char):\n",
    "  return pad_sequences(\n",
    "        char_indice, maxlen=max_len_char, padding='post', value = 0)\n",
    "\n",
    "# 각 단어를 문자 시퀀스로 변환 후 패딩 진행\n",
    "def integer_coding(sentences):\n",
    "  char_data = []\n",
    "  for ts in sentences:\n",
    "    word_indice = [word_to_index[t] for t in ts]\n",
    "    char_indice = [[char_to_index[char] for char in t]  \n",
    "                                          for t in ts]\n",
    "    char_indice = padding_char_indice(char_indice, max_len_char)\n",
    "\n",
    "    for chars_of_token in char_indice:\n",
    "      if len(chars_of_token) > max_len_char:\n",
    "        continue\n",
    "    char_data.append(char_indice)\n",
    "  return char_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64532e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 단위 정수 인코딩 결과\n",
    "X_char_data = integer_coding(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215dd59",
   "metadata": {},
   "source": [
    "동일한 문장에 대해서 단어 단위 정수 인코딩과 문자 단위 정수 인코딩의 차이를 확인해봅시다. \n",
    "\n",
    "첫번째 샘플은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01217bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 문장 :\n",
      "['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩 이전의 기존 문장\n",
    "print('기존 문장 :')\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8479a58",
   "metadata": {},
   "source": [
    "위 문장을 정수 인코딩 및 패딩한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218839df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 단위 정수 인코딩 :\n",
      "[ 254    6  967   16 1795  238  468    7  523    2  129    5   61    9\n",
      "  571    2  833    6  186   90   22   15   56    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 정수 인코딩 + 패딩\n",
    "print('단어 단위 정수 인코딩 :')\n",
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6716a0",
   "metadata": {},
   "source": [
    "254는 기존의 thousands, 6은 기존의 of에 해당됩니다. 해당 샘플을 문자 단위 정수 인코딩한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6aad103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 단위 정수 인코딩 :\n",
      "[[53 41 48 54 52 34 47 37 52  0  0  0  0  0  0]\n",
      " [48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [37 38 46 48 47 52 53 51 34 53 48 51 52  0  0]\n",
      " [41 34 55 38  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [46 34 51 36 41 38 37  0  0  0  0  0  0  0  0]\n",
      " [53 41 51 48 54 40 41  0  0  0  0  0  0  0  0]\n",
      " [45 48 47 37 48 47  0  0  0  0  0  0  0  0  0]\n",
      " [53 48  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [49 51 48 53 38 52 53  0  0  0  0  0  0  0  0]\n",
      " [53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [56 34 51  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [42 47  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [42 51 34 50  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [34 47 37  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [37 38 46 34 47 37  0  0  0  0  0  0  0  0  0]\n",
      " [53 41 38  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [56 42 53 41 37 51 34 56 34 45  0  0  0  0  0]\n",
      " [48 39  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [35 51 42 53 42 52 41  0  0  0  0  0  0  0  0]\n",
      " [53 51 48 48 49 52  0  0  0  0  0  0  0  0  0]\n",
      " [39 51 48 46  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [53 41 34 53  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [36 48 54 47 53 51 58  0  0  0  0  0  0  0  0]\n",
      " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# 문자 단위 정수 인코딩\n",
    "print('문자 단위 정수 인코딩 :')\n",
    "print(X_char_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dad7e4",
   "metadata": {},
   "source": [
    "위 출력 결과에서 각 행은 각 단어를 의미합니다. 가령, thousands는 첫번째 행 [53 41 48 54 52 34 47 37 52 0 0 0 0 0 0]에 해당됩니다. \n",
    "\n",
    "단어의 최대 길이를 15(max_len_char)로 제한하였으므로, 길이가 15보다 짧은 단어는 뒤에 0으로 패딩됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fd785",
   "metadata": {},
   "source": [
    "X_data는 뒤에 0으로 패딩되어 길이가 70인 것에 비해 X_char_data는 현재 0번 단어는 무시되어 길이가 70이 아닙니다. 다시 말해 위 출력 결과에서 행의 개수가 70이 아닌 상태입니다. \n",
    "\n",
    "길이 70으로 맞춰주기 위해서 문장 길이 방향으로도 패딩을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2396fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_data = pad_sequences(X_char_data, maxlen=max_len, \n",
    "                            padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f842b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train, X_char_test, _, _ = train_test_split(X_char_data, y_data, \n",
    "                                                   test_size=.2, \n",
    "                                                   random_state=777)\n",
    "\n",
    "X_char_train = np.array(X_char_train)\n",
    "X_char_test = np.array(X_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0deb57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 150  928  361   17 2624    9 4131 3567    9    8 2893 1250  880  107\n",
      "    3    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f57fb675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soldiers\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035dd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s o l d i e r s PAD PAD PAD PAD PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_char[index] for index in X_char_train[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a4860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 문장의 크기 : (38367, 70)\n",
      "훈련 샘플 레이블의 크기 : (38367, 70, 18)\n",
      "훈련 샘플 char 데이터의 크기 : (38367, 70, 15)\n",
      "테스트 샘플 문장의 크기 : (9592, 70)\n",
      "테스트 샘플 레이블의 크기 : (9592, 70, 18)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('훈련 샘플 char 데이터의 크기 : {}'.format(X_char_train.shape))\n",
    "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd388412",
   "metadata": {},
   "source": [
    "### 2. BiLSTM-CNN을 이용한 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9ce9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "270/270 [==============================] - 24s 62ms/step - loss: 0.2080 - acc: 0.9470 - val_loss: 0.0901 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97526, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 2/15\n",
      "270/270 [==============================] - 16s 59ms/step - loss: 0.0656 - acc: 0.9814 - val_loss: 0.0523 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97526 to 0.98474, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 3/15\n",
      "270/270 [==============================] - 16s 60ms/step - loss: 0.0457 - acc: 0.9867 - val_loss: 0.0464 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98474 to 0.98609, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 4/15\n",
      "270/270 [==============================] - 16s 60ms/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.0422 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98609 to 0.98715, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 5/15\n",
      "270/270 [==============================] - 16s 59ms/step - loss: 0.0337 - acc: 0.9898 - val_loss: 0.0410 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98715 to 0.98723, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 6/15\n",
      "270/270 [==============================] - 16s 61ms/step - loss: 0.0304 - acc: 0.9906 - val_loss: 0.0409 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98723 to 0.98745, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 7/15\n",
      "270/270 [==============================] - 16s 60ms/step - loss: 0.0281 - acc: 0.9913 - val_loss: 0.0408 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98745 to 0.98782, saving model to results\\bilstm_cnn_1307.h5\n",
      "Epoch 8/15\n",
      "270/270 [==============================] - 16s 60ms/step - loss: 0.0262 - acc: 0.9918 - val_loss: 0.0419 - val_acc: 0.9874\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98782\n",
      "Epoch 9/15\n",
      "270/270 [==============================] - 16s 59ms/step - loss: 0.0245 - acc: 0.9922 - val_loss: 0.0409 - val_acc: 0.9878\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98782\n",
      "Epoch 10/15\n",
      "270/270 [==============================] - 16s 59ms/step - loss: 0.0230 - acc: 0.9926 - val_loss: 0.0422 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98782\n",
      "Epoch 11/15\n",
      "270/270 [==============================] - 16s 59ms/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0426 - val_acc: 0.9873\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98782\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from keras_crf import CRFModel\n",
    "\n",
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.5\n",
    "hidden_units = 256\n",
    "num_filters = 30\n",
    "kernel_size = 3\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(shape=(None,),dtype='int32', name='words_input')\n",
    "word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(shape=(None, max_len_char,), name='char_input')\n",
    "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_ids)\n",
    "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
    "\n",
    "# char 임베딩에 대해서는 Conv1D 수행\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, \n",
    "                                    padding='same', activation='tanh', \n",
    "                                    strides=1))(dropout)\n",
    "maxpool_out = TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
    "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
    "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
    "\n",
    "# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Bidirectional(LSTM(hidden_units, return_sequences=True, dropout=dropout_ratio))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='softmax'))(output)\n",
    "\n",
    "model = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('results/bilstm_cnn_1307.h5', monitor='val_acc', mode='max', \n",
    "                     verbose=1, save_best_only=True)\n",
    "\n",
    "# history = model.fit([X_train, X_char_train], y_train, batch_size=128, \n",
    "#                     epochs=15, validation_split=0.1, verbose=1, \n",
    "#                     callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc5ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('results/bilstm_cnn_1307.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b40bd8",
   "metadata": {},
   "source": [
    "테스트 데이터의 13번 인덱스의 샘플에 대해서 예측해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "618c2ed8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  |예측값\n",
      "-----------------------------------\n",
      "the              : O       O\n",
      "statement        : O       O\n",
      "came             : O       O\n",
      "as               : O       O\n",
      "u.n.             : B-org   B-org\n",
      "secretary-general: I-org   I-org\n",
      "kofi             : B-per   B-per\n",
      "annan            : I-per   I-per\n",
      "met              : O       O\n",
      "with             : O       O\n",
      "officials        : O       O\n",
      "in               : O       O\n",
      "amman            : B-geo   B-geo\n",
      "to               : O       O\n",
      "discuss          : O       O\n",
      "wednesday        : B-tim   B-tim\n",
      "'s               : O       O\n",
      "attacks          : O       O\n",
      ".                : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])\n",
    "\n",
    "y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경.\n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], \n",
    "                                      index_to_ner[tag], \n",
    "                                      index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6747da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "    for sequence in sequences:\n",
    "        word_sequence = []\n",
    "        # 시퀀스로부터 확률 벡터 또는 원-핫 벡터를 하나씩 꺼낸다.\n",
    "        for pred in sequence:\n",
    "            # 정수로 변환. 예를 들어 pred가 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            pred_index = np.argmax(pred)            \n",
    "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
    "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
    "        result.append(word_sequence)\n",
    "    return result\n",
    "\n",
    "def sequences_to_tag_for_crf(sequences): \n",
    "    result = []\n",
    "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "    for sequence in sequences: \n",
    "        word_sequence = []\n",
    "        # 시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다.\n",
    "        for pred_index in sequence:\n",
    "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
    "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
    "        result.append(word_sequence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0968053",
   "metadata": {},
   "source": [
    "예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fb3be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 79.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00        63\n",
      "         eve       0.92      0.21      0.34        52\n",
      "         geo       0.84      0.84      0.84      7620\n",
      "         gpe       0.95      0.94      0.95      3145\n",
      "         nat       0.00      0.00      0.00        37\n",
      "         org       0.62      0.55      0.58      4033\n",
      "         per       0.72      0.75      0.73      3545\n",
      "         tim       0.87      0.82      0.85      4067\n",
      "\n",
      "   micro avg       0.80      0.78      0.79     22562\n",
      "   macro avg       0.61      0.51      0.54     22562\n",
      "weighted avg       0.80      0.78      0.79     22562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])\n",
    "pred_tags = sequences_to_tag(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d9eb5",
   "metadata": {},
   "source": [
    "### 3. BiLSTM-CNN-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a841a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "270/270 [==============================] - 78s 274ms/step - decode_sequence_accuracy: 0.9400 - loss: 19.4026 - val_decode_sequence_accuracy: 0.9668 - val_loss: 8.0164\n",
      "\n",
      "Epoch 00001: val_decode_sequence_accuracy improved from -inf to 0.96683, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 2/15\n",
      "270/270 [==============================] - 72s 268ms/step - decode_sequence_accuracy: 0.9787 - loss: 5.0440 - val_decode_sequence_accuracy: 0.9834 - val_loss: 3.7907\n",
      "\n",
      "Epoch 00002: val_decode_sequence_accuracy improved from 0.96683 to 0.98342, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 3/15\n",
      "270/270 [==============================] - 73s 272ms/step - decode_sequence_accuracy: 0.9860 - loss: 2.9955 - val_decode_sequence_accuracy: 0.9851 - val_loss: 3.0915\n",
      "\n",
      "Epoch 00003: val_decode_sequence_accuracy improved from 0.98342 to 0.98508, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 4/15\n",
      "270/270 [==============================] - 74s 273ms/step - decode_sequence_accuracy: 0.9878 - loss: 2.4211 - val_decode_sequence_accuracy: 0.9857 - val_loss: 2.8819\n",
      "\n",
      "Epoch 00004: val_decode_sequence_accuracy improved from 0.98508 to 0.98568, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 5/15\n",
      "270/270 [==============================] - 72s 267ms/step - decode_sequence_accuracy: 0.9889 - loss: 2.0887 - val_decode_sequence_accuracy: 0.9863 - val_loss: 2.6439\n",
      "\n",
      "Epoch 00005: val_decode_sequence_accuracy improved from 0.98568 to 0.98630, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 6/15\n",
      "270/270 [==============================] - 73s 271ms/step - decode_sequence_accuracy: 0.9896 - loss: 1.8414 - val_decode_sequence_accuracy: 0.9861 - val_loss: 2.6650\n",
      "\n",
      "Epoch 00006: val_decode_sequence_accuracy did not improve from 0.98630\n",
      "Epoch 7/15\n",
      "270/270 [==============================] - 72s 266ms/step - decode_sequence_accuracy: 0.9901 - loss: 1.6721 - val_decode_sequence_accuracy: 0.9870 - val_loss: 2.4306\n",
      "\n",
      "Epoch 00007: val_decode_sequence_accuracy improved from 0.98630 to 0.98701, saving model to results/bilstm_cnn_crf_1307\\cp.ckpt\n",
      "Epoch 8/15\n",
      "270/270 [==============================] - 72s 266ms/step - decode_sequence_accuracy: 0.9906 - loss: 1.5360 - val_decode_sequence_accuracy: 0.9868 - val_loss: 2.4600\n",
      "\n",
      "Epoch 00008: val_decode_sequence_accuracy did not improve from 0.98701\n",
      "Epoch 9/15\n",
      "270/270 [==============================] - 73s 269ms/step - decode_sequence_accuracy: 0.9909 - loss: 1.4286 - val_decode_sequence_accuracy: 0.9869 - val_loss: 2.4922\n",
      "\n",
      "Epoch 00009: val_decode_sequence_accuracy did not improve from 0.98701\n",
      "Epoch 10/15\n",
      "270/270 [==============================] - 73s 272ms/step - decode_sequence_accuracy: 0.9911 - loss: 1.3256 - val_decode_sequence_accuracy: 0.9864 - val_loss: 2.5602\n",
      "\n",
      "Epoch 00010: val_decode_sequence_accuracy did not improve from 0.98701\n",
      "Epoch 11/15\n",
      "270/270 [==============================] - 73s 271ms/step - decode_sequence_accuracy: 0.9914 - loss: 1.2284 - val_decode_sequence_accuracy: 0.9861 - val_loss: 2.5688\n",
      "\n",
      "Epoch 00011: val_decode_sequence_accuracy did not improve from 0.98701\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.5\n",
    "hidden_units = 256\n",
    "num_filters = 30\n",
    "kernel_size = 3\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(shape=(None,),dtype='int32')\n",
    "word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(shape=(None, max_len_char,))\n",
    "embed_char_out = TimeDistributed(Embedding(len(char_to_index), char_embedding_dim, \n",
    "                                           embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)))(char_ids)\n",
    "dropout = Dropout(dropout_ratio)(embed_char_out)\n",
    "\n",
    "# char 임베딩에 대해서는 Conv1D 수행\n",
    "conv1d_out = TimeDistributed(Conv1D(kernel_size=kernel_size, filters=num_filters, \n",
    "                                    padding='same', activation='tanh', \n",
    "                                    strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(max_len_char))(conv1d_out)\n",
    "char_embeddings = TimeDistributed(Flatten())(maxpool_out)\n",
    "char_embeddings = Dropout(dropout_ratio)(char_embeddings)\n",
    "\n",
    "# char 임베딩을 Conv1D 수행한 뒤에 단어 임베딩과 연결\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Bidirectional(LSTM(hidden_units, return_sequences=True, \n",
    "                            dropout=dropout_ratio))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
    "\n",
    "base = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model = CRFModel(base, tag_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('results/bilstm_cnn_crf_1307/cp.ckpt', \n",
    "                     monitor='val_decode_sequence_accuracy', \n",
    "                     mode='max', verbose=1, save_best_only=True, \n",
    "                     save_weights_only=True)\n",
    "\n",
    "# history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, \n",
    "#                     epochs=15, validation_split=0.1, \n",
    "#                     callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1910f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19207635da0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('results/bilstm_cnn_crf_1307/cp.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ce4cd",
   "metadata": {},
   "source": [
    "테스트 데이터의 13번 인덱스의 샘플에 대해서 예측해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c71178d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  |예측값\n",
      "-----------------------------------\n",
      "the              : O       O\n",
      "statement        : O       O\n",
      "came             : O       O\n",
      "as               : O       O\n",
      "u.n.             : B-org   B-org\n",
      "secretary-general: I-org   I-org\n",
      "kofi             : B-per   B-per\n",
      "annan            : I-per   I-per\n",
      "met              : O       O\n",
      "with             : O       O\n",
      "officials        : O       O\n",
      "in               : O       O\n",
      "amman            : B-geo   B-geo\n",
      "to               : O       O\n",
      "discuss          : O       O\n",
      "wednesday        : B-tim   B-tim\n",
      "'s               : O       O\n",
      "attacks          : O       O\n",
      ".                : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0] \n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], \n",
    "                                      index_to_ner[tag], \n",
    "                                      index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd256e",
   "metadata": {},
   "source": [
    "예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70b6876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 80.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00        63\n",
      "         eve       0.00      0.00      0.00        52\n",
      "         geo       0.80      0.88      0.84      7620\n",
      "         gpe       0.95      0.94      0.94      3145\n",
      "         nat       0.00      0.00      0.00        37\n",
      "         org       0.70      0.54      0.61      4033\n",
      "         per       0.76      0.75      0.75      3545\n",
      "         tim       0.88      0.85      0.86      4067\n",
      "\n",
      "   micro avg       0.82      0.79      0.80     22562\n",
      "   macro avg       0.51      0.49      0.50     22562\n",
      "weighted avg       0.81      0.79      0.80     22562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])[0]\n",
    "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98e447",
   "metadata": {},
   "source": [
    "### 4. BiLSTM-BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e48bfd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "270/270 [==============================] - 84s 291ms/step - decode_sequence_accuracy: 0.9247 - loss: 24.5613 - val_decode_sequence_accuracy: 0.9708 - val_loss: 7.5368\n",
      "\n",
      "Epoch 00001: val_decode_sequence_accuracy improved from -inf to 0.97080, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 2/15\n",
      "270/270 [==============================] - 76s 282ms/step - decode_sequence_accuracy: 0.9786 - loss: 5.1610 - val_decode_sequence_accuracy: 0.9811 - val_loss: 4.3224\n",
      "\n",
      "Epoch 00002: val_decode_sequence_accuracy improved from 0.97080 to 0.98111, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 3/15\n",
      "270/270 [==============================] - 76s 282ms/step - decode_sequence_accuracy: 0.9843 - loss: 3.3460 - val_decode_sequence_accuracy: 0.9842 - val_loss: 3.3597\n",
      "\n",
      "Epoch 00003: val_decode_sequence_accuracy improved from 0.98111 to 0.98415, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 4/15\n",
      "270/270 [==============================] - 77s 284ms/step - decode_sequence_accuracy: 0.9872 - loss: 2.5703 - val_decode_sequence_accuracy: 0.9857 - val_loss: 2.8144\n",
      "\n",
      "Epoch 00004: val_decode_sequence_accuracy improved from 0.98415 to 0.98572, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 5/15\n",
      "270/270 [==============================] - 77s 284ms/step - decode_sequence_accuracy: 0.9888 - loss: 2.1163 - val_decode_sequence_accuracy: 0.9864 - val_loss: 2.6662\n",
      "\n",
      "Epoch 00005: val_decode_sequence_accuracy improved from 0.98572 to 0.98638, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 6/15\n",
      "270/270 [==============================] - 77s 286ms/step - decode_sequence_accuracy: 0.9899 - loss: 1.8252 - val_decode_sequence_accuracy: 0.9863 - val_loss: 2.5104\n",
      "\n",
      "Epoch 00006: val_decode_sequence_accuracy did not improve from 0.98638\n",
      "Epoch 7/15\n",
      "270/270 [==============================] - 77s 286ms/step - decode_sequence_accuracy: 0.9906 - loss: 1.6086 - val_decode_sequence_accuracy: 0.9867 - val_loss: 2.4829\n",
      "\n",
      "Epoch 00007: val_decode_sequence_accuracy improved from 0.98638 to 0.98669, saving model to results/bilstm_bilstm_crf_1307\\cp.ckpt\n",
      "Epoch 8/15\n",
      "270/270 [==============================] - 79s 292ms/step - decode_sequence_accuracy: 0.9913 - loss: 1.4377 - val_decode_sequence_accuracy: 0.9864 - val_loss: 2.4572\n",
      "\n",
      "Epoch 00008: val_decode_sequence_accuracy did not improve from 0.98669\n",
      "Epoch 9/15\n",
      "270/270 [==============================] - 77s 286ms/step - decode_sequence_accuracy: 0.9919 - loss: 1.2998 - val_decode_sequence_accuracy: 0.9864 - val_loss: 2.5800\n",
      "\n",
      "Epoch 00009: val_decode_sequence_accuracy did not improve from 0.98669\n",
      "Epoch 10/15\n",
      "270/270 [==============================] - 76s 283ms/step - decode_sequence_accuracy: 0.9922 - loss: 1.1792 - val_decode_sequence_accuracy: 0.9863 - val_loss: 2.5628\n",
      "\n",
      "Epoch 00010: val_decode_sequence_accuracy did not improve from 0.98669\n",
      "Epoch 11/15\n",
      "270/270 [==============================] - 78s 288ms/step - decode_sequence_accuracy: 0.9927 - loss: 1.0725 - val_decode_sequence_accuracy: 0.9855 - val_loss: 2.9441\n",
      "\n",
      "Epoch 00011: val_decode_sequence_accuracy did not improve from 0.98669\n",
      "Epoch 12/15\n",
      "270/270 [==============================] - 79s 291ms/step - decode_sequence_accuracy: 0.9931 - loss: 0.9760 - val_decode_sequence_accuracy: 0.9859 - val_loss: 2.8109\n",
      "\n",
      "Epoch 00012: val_decode_sequence_accuracy did not improve from 0.98669\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "char_embedding_dim = 64\n",
    "dropout_ratio = 0.3\n",
    "hidden_units = 64\n",
    "\n",
    "# 단어 임베딩\n",
    "word_ids = Input(batch_shape=(None, None), dtype='int32')\n",
    "word_embeddings = Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        name='word_embedding')(word_ids)\n",
    "\n",
    "# char 임베딩\n",
    "char_ids = Input(batch_shape=(None, None, None), dtype='int32')\n",
    "char_embeddings = Embedding(input_dim=(len(char_to_index)), output_dim=char_embedding_dim,\n",
    "                                        embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5),\n",
    "                                        name='char_embedding')(char_ids)\n",
    "char_embeddings = TimeDistributed(Bidirectional(LSTM(hidden_units)))(char_embeddings)\n",
    "\n",
    "# char 임베딩을 BiLSTM을 통과 시켜 단어 벡터를 얻고 단어 임베딩과 연결\n",
    "output = concatenate([word_embeddings, char_embeddings])\n",
    "\n",
    "# 연결한 벡터를 가지고 문장의 길이만큼 LSTM을 수행\n",
    "output = Dropout(dropout_ratio)(output)\n",
    "output = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(output)\n",
    "\n",
    "# 출력층\n",
    "output = TimeDistributed(Dense(tag_size, activation='relu'))(output)\n",
    "\n",
    "base = Model(inputs=[word_ids, char_ids], outputs=[output])\n",
    "model = CRFModel(base, tag_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('results/bilstm_bilstm_crf_1307/cp.ckpt', \n",
    "                     monitor='val_decode_sequence_accuracy', \n",
    "                     mode='max', verbose=1, save_best_only=True, \n",
    "                     save_weights_only=True)\n",
    "\n",
    "# history = model.fit([X_train, X_char_train], y_train_int, batch_size=128, \n",
    "#                     epochs=15, validation_split=0.1, \n",
    "#                     callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d2fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1936f365e48>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('results/bilstm_bilstm_crf_1307/cp.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e424c4",
   "metadata": {},
   "source": [
    "테스트 데이터의 13번 인덱스의 샘플에 대해서 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a9fab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  |예측값\n",
      "-----------------------------------\n",
      "the              : O       O\n",
      "statement        : O       O\n",
      "came             : O       O\n",
      "as               : O       O\n",
      "u.n.             : B-org   B-org\n",
      "secretary-general: I-org   I-org\n",
      "kofi             : B-per   B-per\n",
      "annan            : I-per   I-per\n",
      "met              : O       O\n",
      "with             : O       O\n",
      "officials        : O       O\n",
      "in               : O       O\n",
      "amman            : B-geo   B-geo\n",
      "to               : O       O\n",
      "discuss          : O       O\n",
      "wednesday        : B-tim   B-tim\n",
      "'s               : O       O\n",
      "attacks          : O       O\n",
      ".                : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict([np.array([X_test[i]]), np.array([X_char_test[i]])])[0]\n",
    "labels = np.argmax(y_test[i], -1) # 원-핫 벡터를 정수 인코딩으로 변경.\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], \n",
    "                                      index_to_ner[tag], \n",
    "                                      index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d1565",
   "metadata": {},
   "source": [
    "예측값과 실제값에 대한 태깅 정보 시퀀스를 얻은 후 F1-score를 계산합니다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bec660b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 80.7%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00        63\n",
      "         eve       1.00      0.13      0.24        52\n",
      "         geo       0.84      0.86      0.85      7620\n",
      "         gpe       0.95      0.94      0.94      3145\n",
      "         nat       0.00      0.00      0.00        37\n",
      "         org       0.65      0.59      0.62      4033\n",
      "         per       0.77      0.74      0.76      3545\n",
      "         tim       0.89      0.83      0.86      4067\n",
      "\n",
      "   micro avg       0.82      0.79      0.81     22562\n",
      "   macro avg       0.64      0.51      0.53     22562\n",
      "weighted avg       0.82      0.79      0.80     22562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict([X_test, X_char_test])[0]\n",
    "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
    "test_tags = sequences_to_tag(y_test)\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073275a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
