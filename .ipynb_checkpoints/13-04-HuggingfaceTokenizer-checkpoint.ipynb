{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a219ffd",
   "metadata": {},
   "source": [
    "# 13. 서브워드 토크나이저(Subword Tokenizer)\n",
    "\n",
    "## 04) 허깅페이스 토크나이저(Huggingface Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fb63e",
   "metadata": {},
   "source": [
    "자연어 처리 스타트업 허깅페이스가 개발한 패키지 tokenizers는 자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공합니다. \n",
    "\n",
    "이번 실습에서는 이 중에서 WordPiece Tokenizer를 실습해보겠습니다. 실습을 위해 우선 tokenizers를 설치합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2af87",
   "metadata": {},
   "source": [
    "```\n",
    "pip install tokenizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b8925",
   "metadata": {},
   "source": [
    "### 1. BERT의 워드피스 토크나이저(BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a37aec",
   "metadata": {},
   "source": [
    "허깅페이스는 해당 토크나이저를 직접 구현하여 tokenizers라는 패키지를 통해 버트워드피스토크나이저(BertWordPieceTokenizer)를 제공합니다.\n",
    "\n",
    "여기서는 네이버 영화 리뷰 데이터를 해당 토크나이저에 학습시키고, 이로부터 서브워드의 단어 집합(Vocabulary)을 얻습니다. 그리고 임의의 문장에 대해서 학습된 토크나이저를 사용하여 토큰화를 진행합니다. 우선 네이버 영화 리뷰 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983587f",
   "metadata": {},
   "source": [
    "```\n",
    "pip install tokenizers==0.10.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f1dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b1b40",
   "metadata": {},
   "source": [
    "ratings.txt라는 파일을 데이터프레임으로 로드한 후, 결측값을 제거하고, 실질적인 리뷰 데이터인 document열에 대해서 naver_review.txt라는 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ef1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_df = pd.read_table('datasets/ratings.txt')\n",
    "naver_df = naver_df.dropna(how='any')\n",
    "with open('datasets/naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0de1b",
   "metadata": {},
   "source": [
    "버트워드피스토크나이저를 설정합니다.\n",
    "\n",
    "각 인자가 의미하는 바는 다음과 같습니다.\n",
    "\n",
    "+ lowercase : 대소문자를 구분 여부. True일 경우 구분하지 않음.\n",
    "+ strip_accents : True일 경우 악센트 제거.\n",
    "\n",
    "    ex) é → e, ô → o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250a2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ca649",
   "metadata": {},
   "source": [
    "네이버 영화 리뷰 데이터를 학습하여 단어 집합을 얻어봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68cb0a",
   "metadata": {},
   "source": [
    "각 인자가 의미하는 바는 다음과 같습니다.\n",
    "\n",
    "+ files : 단어 집합을 얻기 위해 학습할 데이터\n",
    "+ vocab_size : 단어 집합의 크기\n",
    "+ limit_alphabet : 병합 전의 초기 토큰의 허용 개수.\n",
    "+ min_frequency : 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579a3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'datasets/naver_review.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445ff02",
   "metadata": {},
   "source": [
    "학습이 다 되었다면 vocab을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca24fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/vocab.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "tokenizer.save_model('datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0349c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>留먮씪�뒗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>留먮컰�뿉�</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>留섏쓣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>留쏅룄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>留앺븯吏�</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "...       ...\n",
       "29995   留먮씪�뒗\n",
       "29996  留먮컰�뿉�\n",
       "29997     留섏쓣\n",
       "29998     留쏅룄\n",
       "29999   留앺븯吏�\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 로드\n",
    "df = pd.read_fwf('datasets/vocab.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19dd58",
   "metadata": {},
   "source": [
    "실제 토큰화를 수행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e82e162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n",
      "정수 인코딩 : [2111, 20631, 3563, 3362, 24681, 7873, 7378]\n",
      "디코딩 : 아 배고픈데 짜장면먹고싶다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62be2c6",
   "metadata": {},
   "source": [
    ".ids는 실질적인 딥 러닝 모델의 입력으로 사용되는 정수 인코딩 결과를 출력합니다. \n",
    "\n",
    "tokens는 해당 토크나이저가 어떻게 토큰화를 진행했는지를 보여줍니다. \n",
    "\n",
    "decode()는 정수 시퀀스를 문자열로 복원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ad5a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\n",
      "정수 인코딩 : [12825, 25647, 3257, 12696, 3379, 10784, 3264]\n",
      "디코딩 : 커피 한잔의 여유를 즐기다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94ceba",
   "metadata": {},
   "source": [
    "### 2. 기타 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb7eb2",
   "metadata": {},
   "source": [
    "이 외 ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer 등이 존재하며 선택에 따라서 사용할 수 있습니다.\n",
    "\n",
    "+ BertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
    "+ CharBPETokenizer : 오리지널 BPE\n",
    "+ ByteLevelBPETokenizer : BPE의 바이트 레벨 버전\n",
    "+ SentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae934306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train('datasets/naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57e02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이</w>', '영화는</w>', '정말</w>', '재미있습니다</w>', '.</w>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharBPETokenizer()\n",
    "tokenizer.train('datasets/naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d03c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
