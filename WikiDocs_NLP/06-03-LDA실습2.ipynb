{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95041c9c",
   "metadata": {},
   "source": [
    "# 06. 토픽 모델링(Topic Modeling)\n",
    "\n",
    "# 3) 잠재 디리클레 할당(LDA) 실습2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d01fcb",
   "metadata": {},
   "source": [
    "### 1) 뉴스 기사 제목 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "386ff3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "094ec6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3704d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f550f",
   "metadata": {},
   "source": [
    "### 2) 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c365b",
   "metadata": {},
   "source": [
    "토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29b6abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text['headline_text'] = text.apply(\n",
    "    lambda row: nltk.word_tokenize(row['headline_text']), \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cad8c2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0  [aba, decides, against, community, broadcastin...\n",
      "1  [act, fire, witnesses, must, be, aware, of, de...\n",
      "2  [a, g, calls, for, infrastructure, protection,...\n",
      "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
      "4  [air, nz, strike, to, affect, australian, trav...\n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4317968",
   "metadata": {},
   "source": [
    "불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b11c9020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(\n",
    "    lambda x: [word for word in x if word not in (stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e39a42d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a09d8",
   "metadata": {},
   "source": [
    "표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e0ffbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seungwon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0       [aba, decide, community, broadcast, licence]\n",
      "1      [act, fire, witness, must, aware, defamation]\n",
      "2      [g, call, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text['headline_text'] = text['headline_text'].apply(\n",
    "    lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b8b85",
   "metadata": {},
   "source": [
    "길이 3이하 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a1ed1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc = text['headline_text'].apply(\n",
    "    lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf89f1",
   "metadata": {},
   "source": [
    "### 3) TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f88f80",
   "metadata": {},
   "source": [
    "역토큰화 (토큰화 작업을 되돌림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ade6735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seungwon\\anaconda3\\envs\\scratch\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d85ebe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       decide community broadcast licence\n",
       "1       fire witness must aware defamation\n",
       "2    call infrastructure protection summit\n",
       "3                   staff aust strike rise\n",
       "4      strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['headline_text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6b1f2",
   "metadata": {},
   "source": [
    "TfidfVectorizer으로 TF-IDF 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23597836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000) # 상위 1,000개의 단어를 보존 \n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599b35f",
   "metadata": {},
   "source": [
    "### 4) 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8261a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10,\n",
    "                                      learning_method='online',\n",
    "                                      random_state=777,\n",
    "                                      max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71b8b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "458c8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00001533e-01 1.00001269e-01 1.00004179e-01 ... 1.00006124e-01\n",
      "  1.00003111e-01 1.00003064e-01]\n",
      " [1.00001199e-01 1.13513398e+03 3.50170830e+03 ... 1.00009349e-01\n",
      "  1.00001896e-01 1.00002937e-01]\n",
      " [1.00001811e-01 1.00001151e-01 1.00003566e-01 ... 1.00002693e-01\n",
      "  1.00002061e-01 7.53381835e+02]\n",
      " ...\n",
      " [1.00001065e-01 1.00001689e-01 1.00003278e-01 ... 1.00006721e-01\n",
      "  1.00004902e-01 1.00004759e-01]\n",
      " [1.00002401e-01 1.00000732e-01 1.00002989e-01 ... 1.00003517e-01\n",
      "  1.00001428e-01 1.00005266e-01]\n",
      " [1.00003427e-01 1.00002313e-01 1.00007340e-01 ... 1.00003732e-01\n",
      "  1.00001207e-01 1.00005153e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "237ad288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\n",
      "Topic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\n",
      "Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\n",
      "Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\n",
      "Topic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\n",
      "Topic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\n",
      "Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\n",
      "Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\n",
      "Topic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\n",
      "Topic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lda_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c6b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
