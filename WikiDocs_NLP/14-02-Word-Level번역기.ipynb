{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67f510b",
   "metadata": {},
   "source": [
    "# 14. RNN을 이용한 인코더-디코더\n",
    "\n",
    "## 2) Word-Level 번역기 만들기(Neural Machine Translation (seq2seq) Tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97564e3",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a8af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import urllib3\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9f5e5",
   "metadata": {},
   "source": [
    "이번 실습에서는 약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17724ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9244628",
   "metadata": {},
   "source": [
    "전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905c353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii(s):\n",
    "    # 프랑스어 악센트(accent) 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 악센트 제거 함수 호출\n",
    "    sent = to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 추가.\n",
    "    # ex) \"I am a student.\" => \"I am a student .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726da27d",
   "metadata": {},
   "source": [
    "전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행합니다. \n",
    "\n",
    "또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, 훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장합니다. \n",
    "\n",
    "입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7890ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"datasets/fra.txt\", \"r\", encoding='UTF8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbba613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :', sents_en_in[:5])\n",
    "print('디코더의 입력 :', sents_fra_in[:5])\n",
    "print('디코더의 레이블 :', sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc506fd",
   "metadata": {},
   "source": [
    "이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. \n",
    "\n",
    "이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedbae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합 생성\n",
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "\n",
    "# 정수 인코딩\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "\n",
    "# 패딩\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002d3d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 8)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :', encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :', decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :', decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e1d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4635, 프랑스어 단어 집합의 크기 : 8117\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afac656",
   "metadata": {},
   "source": [
    "'단어로부터 정수를 얻는 딕셔너리'와 '정수로부터 단어를 얻는 딕셔너리'를 각각 만들어줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ae97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "\n",
    "index_to_src = tokenizer_en.index_word\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89f937",
   "metadata": {},
   "source": [
    "테스트 데이터를 분리하기 전 데이터를 섞어줍니다. 이를 위해서 순서가 섞인 정수 시퀀스 리스트를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b9c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [22312  2833  8340 ... 23891 32712 12890]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292bcc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea534419",
   "metadata": {},
   "source": [
    "훈련 데이터의 10%를 테스트 데이터로 분리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5fd83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0a4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c534d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 8)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 8)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf46d56",
   "metadata": {},
   "source": [
    "### 2. 기계 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e377ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd65005",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff274a60",
   "metadata": {},
   "source": [
    "인코더를 설계합니다.\n",
    "\n",
    "인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴합니다.\n",
    "\n",
    "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. \n",
    "\n",
    "encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 예정입니다. 이것이 앞서 배운 **컨텍스트 벡터**입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed77661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "\n",
    "# 인코더의 은닉 상태와 셀 상태를 저장\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec34be",
   "metadata": {},
   "source": [
    "디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻습니다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다.\n",
    "\n",
    "디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b98efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 \n",
    "# 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de7793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 16s 37ms/step - loss: 3.4233 - acc: 0.6087 - val_loss: 2.0426 - val_acc: 0.6179\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.8796 - acc: 0.6719 - val_loss: 1.7558 - val_acc: 0.7375\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.6727 - acc: 0.7440 - val_loss: 1.6078 - val_acc: 0.7491\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.5407 - acc: 0.7553 - val_loss: 1.4977 - val_acc: 0.7585\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.4427 - acc: 0.7626 - val_loss: 1.4241 - val_acc: 0.7684\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.3639 - acc: 0.7769 - val_loss: 1.3529 - val_acc: 0.7840\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 1.2894 - acc: 0.7907 - val_loss: 1.2896 - val_acc: 0.7953\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 1.2246 - acc: 0.8013 - val_loss: 1.2398 - val_acc: 0.8022\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 1.1682 - acc: 0.8097 - val_loss: 1.1931 - val_acc: 0.8097\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.1159 - acc: 0.8171 - val_loss: 1.1526 - val_acc: 0.8153\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.0686 - acc: 0.8230 - val_loss: 1.1183 - val_acc: 0.8210\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 1.0245 - acc: 0.8288 - val_loss: 1.0861 - val_acc: 0.8244\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.9847 - acc: 0.8334 - val_loss: 1.0563 - val_acc: 0.8286\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.9469 - acc: 0.8378 - val_loss: 1.0297 - val_acc: 0.8323\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.9108 - acc: 0.8419 - val_loss: 1.0046 - val_acc: 0.8341\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.8771 - acc: 0.8455 - val_loss: 0.9825 - val_acc: 0.8367\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.8445 - acc: 0.8490 - val_loss: 0.9613 - val_acc: 0.8388\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.8153 - acc: 0.8519 - val_loss: 0.9426 - val_acc: 0.8400\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.7865 - acc: 0.8549 - val_loss: 0.9264 - val_acc: 0.8420\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.7596 - acc: 0.8578 - val_loss: 0.9111 - val_acc: 0.8441\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.7348 - acc: 0.8601 - val_loss: 0.8971 - val_acc: 0.8451\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.7114 - acc: 0.8626 - val_loss: 0.8847 - val_acc: 0.8464\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.6883 - acc: 0.8652 - val_loss: 0.8751 - val_acc: 0.8480\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6670 - acc: 0.8675 - val_loss: 0.8658 - val_acc: 0.8490\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6478 - acc: 0.8695 - val_loss: 0.8526 - val_acc: 0.8508\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.6274 - acc: 0.8720 - val_loss: 0.8444 - val_acc: 0.8522\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.6097 - acc: 0.8739 - val_loss: 0.8387 - val_acc: 0.8526\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 7s 31ms/step - loss: 0.5922 - acc: 0.8761 - val_loss: 0.8335 - val_acc: 0.8545\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.5745 - acc: 0.8785 - val_loss: 0.8252 - val_acc: 0.8545\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.5582 - acc: 0.8809 - val_loss: 0.8160 - val_acc: 0.8560\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.5430 - acc: 0.8828 - val_loss: 0.8122 - val_acc: 0.8570\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.5290 - acc: 0.8847 - val_loss: 0.8067 - val_acc: 0.8577\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.5125 - acc: 0.8876 - val_loss: 0.8023 - val_acc: 0.8580\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4998 - acc: 0.8891 - val_loss: 0.7980 - val_acc: 0.8588\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4861 - acc: 0.8909 - val_loss: 0.7935 - val_acc: 0.8600\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4746 - acc: 0.8929 - val_loss: 0.7915 - val_acc: 0.8597\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4612 - acc: 0.8952 - val_loss: 0.7881 - val_acc: 0.8610\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4495 - acc: 0.8970 - val_loss: 0.7839 - val_acc: 0.8616\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4381 - acc: 0.8991 - val_loss: 0.7811 - val_acc: 0.8620\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.4268 - acc: 0.9012 - val_loss: 0.7786 - val_acc: 0.8625\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4161 - acc: 0.9029 - val_loss: 0.7757 - val_acc: 0.8632\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.4058 - acc: 0.9050 - val_loss: 0.7750 - val_acc: 0.8630\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3961 - acc: 0.9067 - val_loss: 0.7736 - val_acc: 0.8645\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3867 - acc: 0.9082 - val_loss: 0.7701 - val_acc: 0.8653\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3769 - acc: 0.9104 - val_loss: 0.7696 - val_acc: 0.8656\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 7s 30ms/step - loss: 0.3685 - acc: 0.9120 - val_loss: 0.7673 - val_acc: 0.8650\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3598 - acc: 0.9136 - val_loss: 0.7692 - val_acc: 0.8660\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3524 - acc: 0.9150 - val_loss: 0.7711 - val_acc: 0.8658\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3457 - acc: 0.9163 - val_loss: 0.7710 - val_acc: 0.8655\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 7s 29ms/step - loss: 0.3371 - acc: 0.9178 - val_loss: 0.7648 - val_acc: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1be52a81518>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91850dc",
   "metadata": {},
   "source": [
    "### 3. seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41792b74",
   "metadata": {},
   "source": [
    "seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다릅니다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. \n",
    "\n",
    "전체적인 번역 단계를 정리하면 아래와 같습니다.\n",
    "\n",
    "1. 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\n",
    "2. 인코더의 은닉 상태와 셀 상태, 그리고 토큰 \\<sos\\>를 디코더로 보냅니다.\n",
    "3. 디코더가 토큰 \\<eos\\>가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8597e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, \n",
    "                                                    initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34e591",
   "metadata": {},
   "source": [
    "테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3329ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "            len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd934d03",
   "metadata": {},
   "source": [
    "결과 확인을 위한 함수를 만듭니다.\n",
    "\n",
    "**seq_to_src**는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환합니다. \n",
    "\n",
    "**seq_to_tar**은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b48548f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if (encoded_word != 0):\n",
    "            sentence += index_to_src[encoded_word] + ' '\n",
    "    return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if (encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence += index_to_tar[encoded_word] + ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bcfad",
   "metadata": {},
   "source": [
    "훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9dc07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : i trusted you . \n",
      "정답문장 : je te fis confiance . \n",
      "번역문장 : je vous ai vus . \n",
      "--------------------------------------------------\n",
      "입력문장 : are you armed ? \n",
      "정답문장 : etes vous armes ? \n",
      "번역문장 : etes vous armes ? \n",
      "--------------------------------------------------\n",
      "입력문장 : we can t find tom . \n",
      "정답문장 : nous n arrivons pas a trouver tom . \n",
      "번역문장 : nous ne pouvons pas y aller au travail . \n",
      "--------------------------------------------------\n",
      "입력문장 : do it right . \n",
      "정답문장 : fais le comme il faut ! \n",
      "번역문장 : fais le le the . \n",
      "--------------------------------------------------\n",
      "입력문장 : you don t count . \n",
      "정답문장 : vous ne comptez pas . \n",
      "번역문장 : vous n en crois pas . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b22444",
   "metadata": {},
   "source": [
    "테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7783dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : we ll all die . \n",
      "정답문장 : nous mourrons tous ! \n",
      "번역문장 : nous comprenons tous . \n",
      "--------------------------------------------------\n",
      "입력문장 : now drink up . \n",
      "정답문장 : vide ton verre maintenant ! \n",
      "번역문장 : va chercher demain . \n",
      "--------------------------------------------------\n",
      "입력문장 : why are you late ? \n",
      "정답문장 : pourquoi etes vous en retard ? \n",
      "번역문장 : pourquoi es tu japonais ? \n",
      "--------------------------------------------------\n",
      "입력문장 : he isn t at home . \n",
      "정답문장 : il n est pas a la maison . \n",
      "번역문장 : il n est pas a la maison . \n",
      "--------------------------------------------------\n",
      "입력문장 : that s perfect . \n",
      "정답문장 : c est parfait . \n",
      "번역문장 : c est parfait . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b77253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
