{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2782ab76",
   "metadata": {},
   "source": [
    "# 10. 워드 임베딩(Word Embedding)\n",
    "\n",
    "## 06) 패스트텍스트(FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41166f",
   "metadata": {},
   "source": [
    "단어를 벡터로 만드는 또 다른 방법.\n",
    "\n",
    "Word2Vec와 FastText와의 가장 큰 차이점이라면 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. \n",
    "\n",
    "내부 단어. 즉, 서브워드(subword)를 고려하여 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105ad5a",
   "metadata": {},
   "source": [
    "### 1. 내부 단어(subword)의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f2bc2",
   "metadata": {},
   "source": [
    "```\n",
    "# n = 3 ~ 6인 경우\n",
    "<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>\n",
    "```\n",
    "\n",
    "단어 apple의 벡터값은 위 벡터값들의 총 합으로 구성합니다.\n",
    "\n",
    "```\n",
    "apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09e4f9",
   "metadata": {},
   "source": [
    "### 2. 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e77fd",
   "metadata": {},
   "source": [
    "가령, FastText에서 birthplace(출생지)란 단어를 학습하지 않은 상태라고 해봅시다. 하지만 다른 단어에서 birth와 place라는 내부 단어가 있었다면, FastText는 birthplace의 벡터를 얻을 수 있습니다. \n",
    "\n",
    "이는 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3c042",
   "metadata": {},
   "source": [
    "### 3. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bbb3a",
   "metadata": {},
   "source": [
    "하지만 FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교하여 비교적 높은 임베딩 벡터값을 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfad8b8",
   "metadata": {},
   "source": [
    "### 4. 실습으로 비교하는 Word2Vec Vs. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8612003",
   "metadata": {},
   "source": [
    "#### 1) Word2Vec\n",
    "\n",
    "```\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "```\n",
    "\n",
    "```\n",
    "KeyError: \"word 'electrofishing' not in vocabulary\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020abea",
   "metadata": {},
   "source": [
    "#### 2) FastText\n",
    "\n",
    "```\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)\n",
    "model.wv.most_similar(\"electrofishing\")\n",
    "```\n",
    "\n",
    "```\n",
    "[('electrolux', 0.7934642434120178), ('electrolyte', 0.78279709815979), ('electro', 0.779127836227417), ('electric', 0.7753111720085144), ('airbus', 0.7648627758026123), ('fukushima', 0.7612422704696655), ('electrochemical', 0.7611693143844604), ('gastric', 0.7483425140380859), ('electroshock', 0.7477173805236816), ('overfishing', 0.7435552477836609)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe98cd",
   "metadata": {},
   "source": [
    "Word2Vec는 학습하지 않은 단어에 대해서 유사한 단어를 찾아내지 못 했지만, FastText는 유사한 단어를 계산해서 출력하고 있음을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f28dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
