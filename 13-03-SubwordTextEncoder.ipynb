{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a963aef",
   "metadata": {},
   "source": [
    "# 13. 서브워드 토크나이저(Subword Tokenizer)\n",
    "\n",
    "## 03) 서브워드텍스트인코더(SubwordTextEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee813d6",
   "metadata": {},
   "source": [
    "SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저입니다. \n",
    "\n",
    "BPE와 유사한 알고리즘인 Wordpiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 서브워드들로 분리할 수 있습니다. \n",
    "\n",
    "SubwordTextEncoder를 통해서 IMDB 영화 리뷰 데이터와 네이버 영화 리뷰 데이터에 대해서 토큰화 작업을 수행해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195b95f",
   "metadata": {},
   "source": [
    "### 1. IMDB 리뷰 토큰화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f116d",
   "metadata": {},
   "source": [
    "```\n",
    "pip install tensorflow_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c633e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc10968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/IMDb_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a30bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89810e",
   "metadata": {},
   "source": [
    "tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 토큰화 할 데이터를 넣어줍니다. \n",
    "\n",
    "이 작업을 통해서 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69044c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00464bc7",
   "metadata": {},
   "source": [
    ".subwords를 통해서 토큰화 된 서브워드들을 확인할 수 있습니다. 100개만 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82887891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4634f63",
   "metadata": {},
   "source": [
    "임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc44697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aefe37",
   "metadata": {},
   "source": [
    "encode()를 통해서 입력한 데이터에 대해서 정수 인코딩을 수행한 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5808e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c7485",
   "metadata": {},
   "source": [
    "임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. \n",
    "\n",
    "디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0603490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
      "기존 문장 : It's mind-blowing to me that this film was even made.\n"
     ]
    }
   ],
   "source": [
    "# train_df에 존재하는 문장 중 일부를 발췌\n",
    "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36d4dc",
   "metadata": {},
   "source": [
    ".vocab_size를 통해 단어 집합의 크기를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2aef9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기(Vocab size) : 8268\n"
     ]
    }
   ],
   "source": [
    "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586e563",
   "metadata": {},
   "source": [
    "디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad4516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "79 ----> even \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e36c0a",
   "metadata": {},
   "source": [
    "이번에는 기존 예제 문장 중 even이라는 단어에 임의로 xyz라는 3개의 글자를 추가해봤습니다. 현재 토크나이저가 even이라는 단어를 이미 하나의 서브워드로 인식하고 있는 상황에서 나머지 xyz를 어떻게 분리하는지를 확인하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29580351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n",
      "기존 문장 : It's mind-blowing to me that this film was evenxyz made.\n"
     ]
    }
   ],
   "source": [
    "# 앞서 실습한 문장에 even 뒤에 임의로 xyz 추가\n",
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9f47dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "7974 ----> even\n",
      "8132 ----> x\n",
      "8133 ----> y\n",
      "997 ----> z \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e733407",
   "metadata": {},
   "source": [
    "evenxyz에서 even을 독립적으로 분리하고 xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로 각각 전부 분리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bbcc6",
   "metadata": {},
   "source": [
    "### 2. 네이버 영화 리뷰 토큰화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e35ae",
   "metadata": {},
   "source": [
    "네이버 영화 리뷰에 대해서도 위에서 IMDB 영화 리뷰에 대해서 수행한 동일한 작업을 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a946f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a9a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('datasets/ratings_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8011b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    5\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcee169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378e128",
   "metadata": {},
   "source": [
    "tfds.features.text.SubwordTextEncoder.build_from_corpus의 인자로 네이버 영화 리뷰 데이터를 넣어서, 서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7ddf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c86ed",
   "metadata": {},
   "source": [
    "토큰화 된 100개의 서브워드들을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df3e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. ', '..', '영화', '이_', '...', '의_', '는_', '도_', '다', ', ', '을_', '고_', '은_', '가_', '에_', '.. ', '한_', '너무_', '정말_', '를_', '고', '게_', '영화_', '지', '... ', '진짜_', '이', '다_', '요', '만_', '? ', '과_', '나', '가', '서_', '지_', '로_', '으로_', '아', '어', '....', '음', '한', '수_', '와_', '도', '네', '그냥_', '나_', '더_', '왜_', '이런_', '면_', '기', '하고_', '보고_', '하는_', '서', '좀_', '리', '자', '스', '안', '! ', '에서_', '영화를_', '미', 'ㅋㅋ', '네요', '시', '주', '라', '는', '오', '없는_', '에', '해', '사', '!!', '영화는_', '마', '잘_', '수', '영화가_', '만', '본_', '로', '그_', '지만_', '대', '은', '비', '의', '일', '개', '있는_', '없다', '함', '구', '하']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a043bd8",
   "metadata": {},
   "source": [
    "encode()를 통해 임의로 선택한 20번 인덱스의 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a68f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\n"
     ]
    }
   ],
   "source": [
    "print(train_data['document'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6404bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b870a06",
   "metadata": {},
   "source": [
    "21번 인덱스 샘플에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩해보겠습니다. 디코딩 할 때는 인코딩할 때 encode()를 사용한 것과 유사하게 decode()를 통해서 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31900b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [570, 892, 36, 584, 159, 7091, 201]\n",
      "기존 문장 : 보면서 웃지 않는 건 불가능하다\n"
     ]
    }
   ],
   "source": [
    "sample_string = train_data['document'][21]\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "257bcbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 ----> 보면서 \n",
      "892 ----> 웃\n",
      "36 ----> 지 \n",
      "584 ----> 않는 \n",
      "159 ----> 건 \n",
      "7091 ----> 불가능\n",
      "201 ----> 하다\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e9268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
